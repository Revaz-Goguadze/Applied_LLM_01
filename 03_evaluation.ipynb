{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Evaluation and Visualization\n",
    "\n",
    "This notebook:\n",
    "1. Loads test dataset and indexes\n",
    "2. Evaluates all four plagiarism detection methods\n",
    "3. Generates comparison charts\n",
    "4. Performs ablation studies\n",
    "5. Analyzes errors and failure patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/uni/applied_LLM/HW1/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from src.retrieval import DenseRetriever, BM25Retriever, HybridRetriever\n",
    "from src.llm import GeminiLLM\n",
    "from src.evaluation import EvaluationMetrics, ErrorAnalyzer\n",
    "from src.visualization import (\n",
    "    plot_comparison_chart,\n",
    "    plot_ablation_study,\n",
    "    plot_confusion_matrices,\n",
    "    plot_cost_vs_performance\n",
    ")\n",
    "from src.config import RANDOM_SEED, DEFAULT_TOP_K, DEFAULT_ALPHA\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset and Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35 test cases\n",
      "Positive examples: 20\n",
      "Negative examples: 15\n",
      "\n",
      "Loading indexes...\n",
      "✓ All systems loaded\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "with open('data/test_dataset.json', 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(test_dataset)} test cases\")\n",
    "print(f\"Positive examples: {sum(1 for tc in test_dataset if tc['is_plagiarism'])}\")\n",
    "print(f\"Negative examples: {sum(1 for tc in test_dataset if not tc['is_plagiarism'])}\")\n",
    "\n",
    "# Load indexes\n",
    "print(\"\\nLoading indexes...\")\n",
    "dense_retriever = DenseRetriever.load(\"indexes/dense_retriever.pkl\")\n",
    "bm25_retriever = BM25Retriever.load(\"indexes/bm25_retriever.pkl\")\n",
    "hybrid_retriever = HybridRetriever(dense_retriever, bm25_retriever)\n",
    "llm = GeminiLLM()\n",
    "print(\"✓ All systems loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Detection Functions\n",
    "\n",
    "(Same as 02_interactive.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Detection functions defined\n"
     ]
    }
   ],
   "source": [
    "def detect_embedding(query_code, threshold=0.85, top_k=5):\n",
    "    results = dense_retriever.retrieve(query_code, top_k=top_k)\n",
    "    if not results:\n",
    "        return {'is_plagiarism': False, 'confidence': 0.0}\n",
    "    top_chunk, max_similarity = results[0]\n",
    "    is_plagiarism = max_similarity >= threshold\n",
    "    return {\n",
    "        'is_plagiarism': is_plagiarism,\n",
    "        'confidence': float(max_similarity * 100),\n",
    "        'max_similarity': float(max_similarity)\n",
    "    }\n",
    "\n",
    "def detect_llm(query_code, max_context_functions=50):\n",
    "    corpus_chunks = dense_retriever.chunks[:max_context_functions]\n",
    "    result = llm.analyze_plagiarism_direct(query_code, corpus_chunks)\n",
    "    return result\n",
    "\n",
    "def detect_rag(query_code, top_k=10):\n",
    "    retrieved = dense_retriever.retrieve(query_code, top_k=top_k)\n",
    "    candidate_chunks = [chunk for chunk, score in retrieved]\n",
    "    result = llm.analyze_plagiarism_with_context(query_code, candidate_chunks)\n",
    "    return result\n",
    "\n",
    "def detect_hybrid_rag(query_code, top_k=10, alpha=0.5):\n",
    "    retrieved = hybrid_retriever.retrieve(query_code, top_k=top_k, alpha=alpha, fusion_method='rrf')\n",
    "    candidate_chunks = [chunk for chunk, score in retrieved]\n",
    "    result = llm.analyze_plagiarism_with_context(query_code, candidate_chunks)\n",
    "    return result\n",
    "\n",
    "print(\"✓ Detection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate All Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_system(detection_func, system_name, test_dataset, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate a detection system on the test dataset.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {system_name}...\")\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for test_case in tqdm(test_dataset, desc=system_name):\n",
    "        query_code = test_case['code']\n",
    "        ground_truth = test_case['is_plagiarism']\n",
    "        \n",
    "        try:\n",
    "            result = detection_func(query_code, **kwargs)\n",
    "            prediction = result.get('is_plagiarism', False)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError on test case {test_case['id']}: {e}\")\n",
    "            prediction = False\n",
    "        \n",
    "        y_true.append(ground_truth)\n",
    "        y_pred.append(prediction)\n",
    "        \n",
    "        # Rate limiting for API calls\n",
    "        if 'llm' in system_name.lower() or 'rag' in system_name.lower():\n",
    "            time.sleep(0.5)  # Avoid rate limits\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = EvaluationMetrics.calculate_metrics(y_true, y_pred)\n",
    "    EvaluationMetrics.print_metrics(metrics, system_name)\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate System 1: Pure Embedding Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Pure Embedding Search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pure Embedding Search: 100%|██████████| 35/35 [00:15<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "evaluation metrics for Pure Embedding Search\n",
      "==================================================\n",
      "precision: 1.0000\n",
      "recall:    0.8500\n",
      "f1 score:  0.9189\n",
      "accuracy:  0.9143\n",
      "\n",
      "confusion matrix:\n",
      "  tp:  17  fn:   3\n",
      "  fp:   0  tn:  15\n",
      "\n",
      "error rates:\n",
      "  false positive rate: 0.0000\n",
      "  false negative rate: 0.1500\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_embedding, pred_embedding = evaluate_system(\n",
    "    detect_embedding,\n",
    "    \"Pure Embedding Search\",\n",
    "    test_dataset,\n",
    "    threshold=0.85\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate System 2: Direct LLM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Direct LLM Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 0: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:   3%|▎         | 1/35 [00:00<00:17,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 1: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:   6%|▌         | 2/35 [00:01<00:16,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 2: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:   9%|▊         | 3/35 [00:01<00:16,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 3: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  11%|█▏        | 4/35 [00:02<00:15,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 4: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  14%|█▍        | 5/35 [00:02<00:15,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 5: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  17%|█▋        | 6/35 [00:03<00:14,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 6: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  20%|██        | 7/35 [00:03<00:14,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 7: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  23%|██▎       | 8/35 [00:04<00:13,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 8: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  26%|██▌       | 9/35 [00:04<00:13,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 9: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  29%|██▊       | 10/35 [00:05<00:12,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 10: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  31%|███▏      | 11/35 [00:05<00:12,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 11: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  34%|███▍      | 12/35 [00:06<00:11,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 12: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  37%|███▋      | 13/35 [00:06<00:11,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 13: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  40%|████      | 14/35 [00:07<00:10,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 14: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  43%|████▎     | 15/35 [00:07<00:10,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 15: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  46%|████▌     | 16/35 [00:08<00:09,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 16: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  49%|████▊     | 17/35 [00:08<00:09,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 17: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  51%|█████▏    | 18/35 [00:09<00:08,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 18: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  54%|█████▍    | 19/35 [00:09<00:08,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 19: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  57%|█████▋    | 20/35 [00:10<00:07,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 20: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  60%|██████    | 21/35 [00:10<00:07,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 21: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  63%|██████▎   | 22/35 [00:11<00:06,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 22: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  66%|██████▌   | 23/35 [00:11<00:06,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 23: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  69%|██████▊   | 24/35 [00:12<00:05,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 24: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  71%|███████▏  | 25/35 [00:12<00:05,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 25: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  74%|███████▍  | 26/35 [00:13<00:04,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 26: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  77%|███████▋  | 27/35 [00:13<00:04,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 27: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  80%|████████  | 28/35 [00:14<00:03,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 28: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  83%|████████▎ | 29/35 [00:14<00:03,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 29: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  86%|████████▌ | 30/35 [00:15<00:02,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 30: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  89%|████████▊ | 31/35 [00:15<00:02,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 31: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  91%|█████████▏| 32/35 [00:16<00:01,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 32: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  94%|█████████▍| 33/35 [00:16<00:01,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 33: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis:  97%|█████████▋| 34/35 [00:17<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error on test case 34: GeminiLLM.analyze_plagiarism_direct() takes 2 positional arguments but 3 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Direct LLM Analysis: 100%|██████████| 35/35 [00:17<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "evaluation metrics for Direct LLM Analysis\n",
      "==================================================\n",
      "precision: 0.0000\n",
      "recall:    0.0000\n",
      "f1 score:  0.0000\n",
      "accuracy:  0.4286\n",
      "\n",
      "confusion matrix:\n",
      "  tp:   0  fn:  20\n",
      "  fp:   0  tn:  15\n",
      "\n",
      "error rates:\n",
      "  false positive rate: 0.0000\n",
      "  false negative rate: 1.0000\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_llm, pred_llm = evaluate_system(\n",
    "    detect_llm,\n",
    "    \"Direct LLM Analysis\",\n",
    "    test_dataset,\n",
    "    max_context_functions=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate System 3: Standard RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Standard RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard RAG:   3%|▎         | 1/35 [00:02<01:21,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation failed (attempt 1): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200, model: gemini-2.0-flash\n",
      "Please retry in 40.145493361s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "generation failed (attempt 2): 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200, model: gemini-2.0-flash\n",
      "Please retry in 38.93538635s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "\n",
      "Error on test case 1: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 200, model: gemini-2.0-flash\n",
      "Please retry in 36.724795415s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 200\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard RAG:   3%|▎         | 1/35 [00:06<03:53,  6.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m metrics_rag, pred_rag = \u001b[43mevaluate_system\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetect_rag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStandard RAG\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mevaluate_system\u001b[39m\u001b[34m(detection_func, system_name, test_dataset, **kwargs)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Rate limiting for API calls\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m system_name.lower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mrag\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m system_name.lower():\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Avoid rate limits\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m     29\u001b[39m metrics = EvaluationMetrics.calculate_metrics(y_true, y_pred)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "metrics_rag, pred_rag = evaluate_system(\n",
    "    detect_rag,\n",
    "    \"Standard RAG\",\n",
    "    test_dataset,\n",
    "    top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate System 4: Hybrid RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_hybrid, pred_hybrid = evaluate_system(\n",
    "    detect_hybrid_rag,\n",
    "    \"Hybrid RAG\",\n",
    "    test_dataset,\n",
    "    top_k=10,\n",
    "    alpha=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "all_results = {\n",
    "    'Pure Embedding': metrics_embedding,\n",
    "    'Direct LLM': metrics_llm,\n",
    "    'Standard RAG': metrics_rag,\n",
    "    'Hybrid RAG': metrics_hybrid\n",
    "}\n",
    "\n",
    "# Plot comparison chart\n",
    "plot_comparison_chart(all_results, save_path=\"results/comparison_chart.png\")\n",
    "\n",
    "# Save results to JSON\n",
    "os.makedirs('results', exist_ok=True)\n",
    "with open('results/evaluation_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Results saved to results/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(all_results, save_path=\"results/confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study 1: Varying k in RAG Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAblation Study: Impact of k on Standard RAG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "k_values = [3, 5, 10, 15, 20]\n",
    "k_results = {\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nTesting k={k}...\")\n",
    "    metrics, _ = evaluate_system(\n",
    "        detect_rag,\n",
    "        f\"RAG (k={k})\",\n",
    "        test_dataset[:15],  # Use subset for speed\n",
    "        top_k=k\n",
    "    )\n",
    "    \n",
    "    k_results['precision'].append(metrics['precision'])\n",
    "    k_results['recall'].append(metrics['recall'])\n",
    "    k_results['f1_score'].append(metrics['f1_score'])\n",
    "    k_results['accuracy'].append(metrics['accuracy'])\n",
    "\n",
    "# Plot results\n",
    "plot_ablation_study(\n",
    "    k_values,\n",
    "    k_results,\n",
    "    save_path=\"results/ablation_k_values.png\",\n",
    "    title=\"Ablation Study: Impact of k on RAG Performance\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Ablation study complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study 2: Fusion Weights in Hybrid RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAblation Study: Fusion Weight (alpha) in Hybrid RAG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "alphas = [0.0, 0.3, 0.5, 0.7, 1.0]  # 0=pure BM25, 1=pure dense\n",
    "alpha_results = {\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"\\nTesting alpha={alpha}...\")\n",
    "    metrics, _ = evaluate_system(\n",
    "        detect_hybrid_rag,\n",
    "        f\"Hybrid RAG (α={alpha})\",\n",
    "        test_dataset[:15],  # Use subset for speed\n",
    "        top_k=10,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    \n",
    "    alpha_results['precision'].append(metrics['precision'])\n",
    "    alpha_results['recall'].append(metrics['recall'])\n",
    "    alpha_results['f1_score'].append(metrics['f1_score'])\n",
    "    alpha_results['accuracy'].append(metrics['accuracy'])\n",
    "\n",
    "# Plot results\n",
    "plot_ablation_study(\n",
    "    alphas,\n",
    "    alpha_results,\n",
    "    save_path=\"results/ablation_alpha_values.png\",\n",
    "    title=\"Ablation Study: Fusion Weight (α) in Hybrid RAG\\n(0=pure BM25, 1=pure Dense)\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Ablation study complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors for each system\n",
    "systems = {\n",
    "    'Pure Embedding': pred_embedding,\n",
    "    'Direct LLM': pred_llm,\n",
    "    'Standard RAG': pred_rag,\n",
    "    'Hybrid RAG': pred_hybrid\n",
    "}\n",
    "\n",
    "for system_name, predictions in systems.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Error Analysis: {system_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    errors = ErrorAnalyzer.analyze_errors(test_dataset, predictions)\n",
    "    ErrorAnalyzer.print_error_analysis(errors)\n",
    "    \n",
    "    # Show sample false positives\n",
    "    if errors['false_positives']:\n",
    "        print(\"\\nSample False Positive:\")\n",
    "        fp = errors['false_positives'][0]\n",
    "        print(f\"Code: {fp['test_case']['code'][:100]}...\")\n",
    "    \n",
    "    # Show sample false negatives\n",
    "    if errors['false_negatives']:\n",
    "        print(\"\\nSample False Negative:\")\n",
    "        fn = errors['false_negatives'][0]\n",
    "        print(f\"Code: {fn['test_case']['code'][:100]}...\")\n",
    "        print(f\"Transformation: {fn['test_case'].get('transformation_type', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost vs Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate relative costs (arbitrary units)\n",
    "systems_list = ['Pure Embedding', 'Standard RAG', 'Hybrid RAG', 'Direct LLM']\n",
    "f1_scores = [\n",
    "    all_results['Pure Embedding']['f1_score'],\n",
    "    all_results['Standard RAG']['f1_score'],\n",
    "    all_results['Hybrid RAG']['f1_score'],\n",
    "    all_results['Direct LLM']['f1_score']\n",
    "]\n",
    "costs = [1, 5, 6, 10]  # Relative costs (embedding < RAG < hybrid < direct LLM)\n",
    "\n",
    "plot_cost_vs_performance(\n",
    "    systems_list,\n",
    "    f1_scores,\n",
    "    costs,\n",
    "    save_path=\"results/cost_vs_performance.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nPerformance Ranking by F1 Score:\")\n",
    "ranked = sorted(all_results.items(), key=lambda x: x[1]['f1_score'], reverse=True)\n",
    "for i, (system, metrics) in enumerate(ranked):\n",
    "    print(f\"{i+1}. {system}: F1={metrics['f1_score']:.4f}, \"\n",
    "          f\"Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "insights = [\n",
    "    \"1. Pure Embedding: Fast and cheap, good for exact/minor copies\",\n",
    "    \"2. Direct LLM: Best context understanding, but expensive and slow\",\n",
    "    \"3. Standard RAG: Balanced approach, depends on retrieval quality\",\n",
    "    \"4. Hybrid RAG: Best of both retrieval methods, slight complexity increase\",\n",
    "    \"\\nTrade-offs:\",\n",
    "    \"- Accuracy ↔ Cost: More LLM usage = higher accuracy but higher cost\",\n",
    "    \"- Accuracy ↔ Speed: Embedding-only is fastest, LLM methods slower\",\n",
    "    \"- Retrieval Quality Impact: RAG performance heavily depends on top-k selection\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE - All results saved to results/\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
